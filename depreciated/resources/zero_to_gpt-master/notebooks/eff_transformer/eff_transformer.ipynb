{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Based on this paper - https://arxiv.org/pdf/1706.03762.pdf\n",
    "# Might want to move layer norm inside the residual block - https://arxiv.org/pdf/2002.04745.pdf\n",
    "# Layer normalization - https://arxiv.org/pdf/1607.06450.pdf\n",
    "# TODO: Investigate learning rate warmup - https://arxiv.org/abs/2002.04745\n",
    "#!pip install torch datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import einops\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "sys.path.append(os.path.abspath(\"../../nnets\"))\n",
    "from net_utils import get_module_list\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "SP_VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Fix dataset to work with new format\n",
    "# TODO: Improve dataset to optimize padding and other things (drop incomplete, etc)\n",
    "from text_data import WikiTextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_wrapper = WikiTextDataset(tokenizer_vocab=SP_VOCAB_SIZE, download_split_pct=\"2%\", model_max_length=128)\n",
    "train_dataset = train_wrapper.process_dataset()\n",
    "train_dataset = train_dataset.train_test_split(test_size=0.005)\n",
    "\n",
    "train_split = train_dataset[\"train\"]\n",
    "train = DataLoader(train_split, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_split = train_dataset[\"test\"]\n",
    "valid = DataLoader(valid_split, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add in ROPE embedding\n",
    "class ROPE(nn.Module):\n",
    "    def __init__(self, embedding_dim, seq_len):\n",
    "        super(ROPE, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.cos_embeds = torch.zeros(seq_len, embedding_dim, device=DEVICE)\n",
    "        self.sin_embeds = torch.zeros(seq_len, embedding_dim, device=DEVICE)\n",
    "\n",
    "        embed_pos = 10000 ** (-2 * torch.ceil((torch.arange(0, embedding_dim) + 1) / 2) / embedding_dim)\n",
    "        for i in range(0, seq_len):\n",
    "            self.cos_embeds[i] = torch.cos((i + 1) * embed_pos)\n",
    "            self.sin_embeds[i] = torch.sin((i + 1) * embed_pos)\n",
    "\n",
    "        self.indices = torch.zeros(self.embedding_dim, device=DEVICE, dtype=torch.long)\n",
    "        self.mask = torch.zeros(self.embedding_dim, device=DEVICE, dtype=torch.int)\n",
    "        for i in range(0, embedding_dim, 2):\n",
    "            self.indices[i] = i + 1\n",
    "            self.indices[i+1] = i\n",
    "\n",
    "            self.mask[i] = -1\n",
    "            self.mask[i+1] = 1\n",
    "\n",
    "\n",
    "    def rotate(self, x):\n",
    "        return x[...,self.indices] * self.mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        current_val = x * self.cos_embeds[:x.shape[-2],:]\n",
    "        next_val = self.rotate(x) * self.sin_embeds[:x.shape[-2],:]\n",
    "        return current_val + next_val\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.head_units = int(input_units/attention_heads)\n",
    "        self.mask = mask\n",
    "\n",
    "        k = math.sqrt(1/self.input_units)\n",
    "        # Drop bias\n",
    "        # Single kv head\n",
    "        self.kv_proj_weight = nn.Parameter(torch.rand(2, input_units, self.head_units) * 2 * k - k)\n",
    "        self.q_proj_weight = nn.Parameter(torch.rand(input_units, self.attention_heads * self.head_units) * 2 * k - k)\n",
    "        self.out_proj_weight = nn.Parameter(torch.rand(self.attention_heads * self.head_units, input_units) * 2 * k - k)\n",
    "\n",
    "        # 1024 is max sequence length\n",
    "        self.rope = ROPE(self.head_units, 1024)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # convert to 4d tensor with batch_size, attn_heads, seq_len, embedding_dim\n",
    "        proj_queries = torch.einsum(\"...se, eo->...so\", queries, self.q_proj_weight)\n",
    "        proj_queries = proj_queries.view(queries.shape[0], queries.shape[1], self.attention_heads, self.head_units).swapaxes(1,2)\n",
    "        proj_queries = self.rope(proj_queries)\n",
    "\n",
    "        proj_keys = torch.einsum(\"...se, eo->...so\", keys, self.kv_proj_weight[0])\n",
    "        proj_keys = proj_keys.view(keys.shape[0], keys.shape[1], self.head_units)\n",
    "        proj_keys = self.rope(proj_keys)\n",
    "\n",
    "        proj_values = torch.einsum(\"...se, eo->...so\", values, self.kv_proj_weight[0])\n",
    "        proj_values = proj_values.view(values.shape[0], values.shape[1], self.head_units)\n",
    "\n",
    "        attention = torch.einsum(\"baqh, bhk->baqk\", proj_queries, torch.transpose(proj_keys, -1, -2)) / np.sqrt(proj_keys.shape[-1])\n",
    "        if self.mask:\n",
    "            # Prevent decoder queries from looking at tokens that come after\n",
    "            # Do this by setting attention to negative infinity, so it is softmaxed to zero in the next step\n",
    "            mask = torch.full((attention.shape[-2], attention.shape[-1]), -torch.inf, device=DEVICE)\n",
    "            attention += torch.triu(mask, diagonal=1)\n",
    "\n",
    "        # Softmax on last dimension\n",
    "        # Sequence-wise softmax, so attention between one sequence and other sequences sums to 1\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        weighted_values = torch.einsum(\"baqk, bke->baqe\", attention, proj_values)\n",
    "\n",
    "        # Swap attention head and sequence axis, then reshape to batch, seq, embedding\n",
    "        weighted_values = weighted_values.swapaxes(1,2).reshape(queries.shape[0], queries.shape[1], -1)\n",
    "        weighted_values = torch.einsum(\"...se, eo->...so\", weighted_values, self.out_proj_weight)\n",
    "        return weighted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units):\n",
    "        super(SwiGLU, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_units, hidden_units, bias=False)\n",
    "        self.linear2 = nn.Linear(input_units, hidden_units, bias=False)\n",
    "        self.linear3 = nn.Linear(hidden_units, input_units, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.linear1(x)\n",
    "        swish = self.sigmoid(x1) * x1\n",
    "        x2 = self.linear2(x)\n",
    "        swiglu = self.linear3(swish * x2)\n",
    "        return swiglu\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, hidden_units=2048, dropout_p=.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.in_attn = MultiHeadAttention(input_units, attention_heads, mask=True)\n",
    "        self.dropouts = get_module_list(2, nn.Dropout, dropout_p)\n",
    "        # Drop bias\n",
    "        self.lns = get_module_list(2, nn.LayerNorm, input_units)\n",
    "        # Switch to swiglu from two linear layers\n",
    "        self.swiglu = SwiGLU(input_units, hidden_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_values = self.dropouts[0](self.in_attn(x, x, x))\n",
    "        # Pre normalization\n",
    "        x = x + self.lns[0](weighted_values)\n",
    "\n",
    "        reprojected = self.dropouts[1](self.swiglu(x))\n",
    "        # Pre normalization\n",
    "        x = x + self.lns[1](reprojected)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, attention_heads, max_len=256, blocks=1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.blocks = blocks\n",
    "        self.dropouts = get_module_list(2, nn.Dropout, .1)\n",
    "        self.decoders = get_module_list(blocks, DecoderBlock, hidden_units, attention_heads)\n",
    "\n",
    "        self.embedding = nn.Parameter(torch.empty(input_units, hidden_units))\n",
    "        nn.init.xavier_uniform_(self.embedding)\n",
    "        self.input_units = input_units\n",
    "\n",
    "    # Tie input output weights\n",
    "    def embed(self, x, reverse=False):\n",
    "        if reverse:\n",
    "            return x @ self.embedding.T\n",
    "        else:\n",
    "            embedded = self.embedding[x.to(torch.long).view(-1)]\n",
    "            return embedded.view(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        dec_outputs = self.dropouts[1](self.embed(x))\n",
    "        for i in range(self.blocks):\n",
    "            dec_outputs = self.decoders[i](dec_outputs)\n",
    "\n",
    "        token_vectors = self.embed(dec_outputs, reverse=True)\n",
    "        return token_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def replace_newline(text):\n",
    "    return text.replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "def generate(sequence, pred, wrapper):\n",
    "    prompts = wrapper.decode_ids(sequence.cpu())\n",
    "    texts = wrapper.decode_ids(torch.argmax(pred, dim=2).cpu())\n",
    "\n",
    "    displays = []\n",
    "    for p, t in zip(prompts, texts):\n",
    "        displays.append(f\"{replace_newline(p)} \\x1b[31m{replace_newline(t)}\\x1b[0m\")\n",
    "    return displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "wandb.init(project=\"decoder-only\", notes=\"Change dataset\", name=\"change-dataset\")\n",
    "\n",
    "# TODO: Profile and improve perf - https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "model = Transformer(SP_VOCAB_SIZE, 512, 8, blocks=6).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=train_wrapper.tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "wandb.watch(model, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "DISPLAY_BATCHES = 2\n",
    "PRINT_VALID = True\n",
    "ACCUMULATE_STEPS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    match_pct = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    start = time.time()\n",
    "    for idx, batch in enumerate(tqdm(train)):\n",
    "        sequence = batch[\"input_ids\"].to(torch.long)\n",
    "        target = torch.roll(sequence, -1, -1)\n",
    "        # TODO: Look into replace this with eos id instead\n",
    "        target[:,-1] = train_wrapper.tokenizer.pad_token_id\n",
    "        pred = model(sequence.to(DEVICE))\n",
    "\n",
    "        loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.reshape(-1).to(DEVICE))\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Accumulate gradients\n",
    "        # This seems to perform worse than no accumulation over a\n",
    "        # small data set.  Test with larger set.\n",
    "        if idx % ACCUMULATE_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "    end = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean_loss = train_loss / len(train) / BATCH_SIZE\n",
    "        wandb.log({\"loss\": mean_loss, \"epoch_time\": end - start})\n",
    "        print(f\"Epoch {epoch} train loss: {mean_loss}\")\n",
    "        sents = generate(sequence, pred, train_wrapper)\n",
    "        for sent in sents[:DISPLAY_BATCHES]:\n",
    "            print(sent)\n",
    "\n",
    "        if PRINT_VALID and epoch % 1 ==0:\n",
    "            # Compute validation loss.  Unless you have a lot of training data, the validation loss won't decrease.\n",
    "            valid_loss = 0\n",
    "            # Deactivate dropout layers\n",
    "            model.eval()\n",
    "            for batch in tqdm(valid):\n",
    "                # Inference token by tokens\n",
    "                sequence = batch[\"input_ids\"].to(torch.long)\n",
    "                target = torch.roll(sequence, -1, -1)\n",
    "                target[:,-1] = train_wrapper.tokenizer.pad_token_id\n",
    "\n",
    "                outputs = sequence[:,:50].to(DEVICE)\n",
    "                # TODO: Investigate memory leak with valid generation\n",
    "                for i in range(sequence.shape[1]):\n",
    "                    pred = model(outputs)\n",
    "                    last_output = torch.argmax(pred, dim=2)\n",
    "                    outputs = torch.cat((outputs, last_output[:,-1:]), dim=1)\n",
    "\n",
    "                loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.reshape(-1).to(DEVICE))\n",
    "                valid_loss += loss.item()\n",
    "            mean_loss = valid_loss / len(valid) / BATCH_SIZE\n",
    "            wandb.log({\"valid_loss\": mean_loss})\n",
    "            print(f\"Valid loss: {mean_loss}\")\n",
    "            sents = generate(sequence, pred, train_wrapper)\n",
    "            for sent in sents[:DISPLAY_BATCHES]:\n",
    "                print(sent)\n",
    "            # Reactivate dropout\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, ) as prof:\n",
    "    model(sequence.to(DEVICE), prev_target.to(DEVICE))\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "x = torch.rand(4, 3)\n",
    "x[torch.tensor([0,1,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
