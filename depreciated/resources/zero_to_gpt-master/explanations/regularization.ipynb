{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and overfitting\n",
    "\n",
    "You may be surprised to learn that the goal of training a neural network is not to minimize the training loss.  This might seem counterintuitive.  We've been training models using gradient descent, where training loss decreases over time.\n",
    "\n",
    "However, the training data is just a sample of the population data.  For example,we have data on house prices, where each row in this dataset represents a single house.  Here are the columns:\n",
    "\n",
    "- `interest`: The interest rate\n",
    "- `vacancy`: The vacancy rate\n",
    "- `cpi`: The consumer price index\n",
    "- `price`: The price of a house\n",
    "- `value`: The value of a house\n",
    "- `adj_price`: The price of a house, adjusted for inflation\n",
    "- `adj_value`: The value of a house, adjusted for inflation\n",
    "\n",
    "We have about `700` different rows of data - this is called our sample.  There are many more houses that aren't in our training data - this is called the population.  What we actually want to do is train the model on our sample of data, but use it to make predictions on the population.\n",
    "\n",
    "If our model makes good predictions in our sample, but bad predictions in the population, then that's called overfitting.  Overfitting happens when the model learns random characteristics of our sample instead of the underlying characteristics of our population. For example, if the vacancy rate is always high before house prices drop in the sample, but not in the population, then this is a random quirk of our sample.\n",
    "\n",
    "Overfitting means that the model won't be useful in the real world.  Neural networks are prone to overfitting, and there are many techniques that have been developed to prevent it.  These techniques are broadly called regularization, which adds constraints or penalties to the model's parameters, in order to encourage it to learn simpler and more generalizable representations.  These will usually increase loss in the sample, but decrease loss in the population.\n",
    "\n",
    "Of course, we don't have access to data from the population.  This means that we need to split our sample up into a training set, a validation set, and a test set.  While we're training the model, we'll evaluate on the validation set.  After we've optimized our training method and parameters, we'll evaluate on the test set.  This ensures that we're not using knowledge of the test set when we tune our parameters (this could cause overfitting to the test set).\n",
    "\n",
    "We'll learn three forms of regularization:\n",
    "\n",
    "- Weight decay, which decreases the magnitude of the weights in the optimizer.  This pushes most of the weights towards zero, which encourages the model to learn simpler representations.\n",
    "- Dropout, which randomly sets activations to zero.  This prevents the model from relying on any single activation, and encourages it to learn more generalizable representations.\n",
    "- Early stopping, which stops training when the validation loss starts to increase.  This prevents overfitting by stopping training before the model starts to memorize the training data.\n",
    "\n",
    "There are other forms of regularization, like data augmentation, but these are the most common when working with text.\n",
    "\n",
    "At the end of this lesson, we'll also cover two techniques that can help with overfitting and convergence, but aren't strictly regularization techniques:\n",
    "\n",
    "- Layer normalization - this technique normalizes the activations to have mean 0 and standard deviation 1, which reduces the magnitude of the values\n",
    "- Residual connections - this is a \"skip-layer\" connection that sums that activations from the previous layer with the activations from the current layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "We'll first load in the data, which is the same from the last lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "sys.path.append(os.path.abspath('../nnets'))\n",
    "from dense import DenseManualUpdate as Dense, forward, backward\n",
    "from csv_data import HousePricesDatasetWrapper\n",
    "import numpy as np\n",
    "from optimizer import Optimizer\n",
    "\n",
    "wrapper = HousePricesDatasetWrapper()\n",
    "train_data, valid_data, test_data = wrapper.get_flat_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a training run\n",
    "\n",
    "As you can see, we split the data into three sets.  We'll use the training set to train the model, the validation set to evaluate the model, and the test set to evaluate the model after we've finished training.\n",
    "\n",
    "We'll write a training loop function, which will allow us to test different types of regularization.  The function:\n",
    "\n",
    "- Sets up a new W&B run for monitoring\n",
    "- Loops through the training data with batch size 1:\n",
    "    - Makes a prediction\n",
    "    - Finds the error\n",
    "    - Computes the gradient\n",
    "    - Updates the parameters\n",
    "- Logs the loss\n",
    "\n",
    "We will again be using Weights & Biases (W&B) to log our training data and plot loss curves.  There is a nice web dashboard where you can see the results of each run.  If you don't want to sign up for a W&B account, you can leave the monitoring code out, but you will need to add in a way to print out the loss at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "# Initialize W&B\n",
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "def training_run(epochs, regularization, layers, optimizer, train_data, valid_data, name=None):\n",
    "    # Initialize a new W&B run\n",
    "    wandb.init(project=\"regularization\",\n",
    "               name=name,\n",
    "               config={\"regularization\": regularization})\n",
    "\n",
    "    # Split the training and valid data into x and y\n",
    "    train_x, train_y = train_data\n",
    "    valid_x, valid_y = valid_data\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i in range(len(train_x)):\n",
    "            # Get the x and y batches\n",
    "            x_batch = train_x[i:(i+1)]\n",
    "            y_batch = train_y[i:(i+1)]\n",
    "            # Make a prediction\n",
    "            pred = forward(x_batch, layers, training=True)\n",
    "\n",
    "            # Run the backward pass\n",
    "            loss = pred - y_batch\n",
    "            layer_grads = backward(loss, layers)\n",
    "            running_loss += np.mean(loss ** 2)\n",
    "\n",
    "            # Run the optimizer\n",
    "            optimizer(layer_grads, layers, 1)\n",
    "\n",
    "        # Calculate and log validation loss\n",
    "        valid_preds = forward(valid_x, layers, training=False)\n",
    "        valid_loss = np.mean((valid_preds - valid_y) ** 2)\n",
    "        train_loss = running_loss / len(train_x)\n",
    "\n",
    "        # Log training metrics\n",
    "        wandb.log({\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "        })\n",
    "\n",
    "    # Mark the run as complete\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also write a function to calculate test set loss.  This will help us evaluate different regularization models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(layers, test_data):\n",
    "    test_x, test_y = test_data\n",
    "    preds = forward(test_x, layers, training=False)\n",
    "    loss = np.mean((preds - test_y) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight decay\n",
    "\n",
    "Weight decay is similar to L2 regularization.  The goal is to shrink the weights towards 0 (reduce the L2 norm of the weights).  This means that the model will be less likely to learn very large weights.  Very large weights can cause overfitting, because it means that the model is putting too much importance on a single value.\n",
    "\n",
    "The difference between L2 regularization and weight decay is small, and has to do with where the weights are shrunk.  In SGD, they are the same, but this is not the case in other optimizers like SGD with momentum.  You can read more about the difference [here](https://arxiv.org/pdf/1711.05101.pdf).\n",
    "\n",
    "As an example, let's look at the weights of a model with and without weight decay.  First, we'll define our SGDW optimizer.  This is SGD, but with weight decay.  The main addition is the line `w_update -= self.decay * layer.weights`.  This multiplies a decay coefficient (usually a number like `.01`) by the weights, then adds it to the weight update.  This will shrink the weights over time.\n",
    "\n",
    "For example, if the decay coefficient is `.1`, and we ignore the gradient, then after the first update, the weights will be `90%` of their original magnitude, `81%` after the second update, and so on.  This means that the shrinkage will be proportional to the size of the weight.  So large weights will be reduced more in absolute terms.\n",
    "\n",
    "Now, we can define the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDW(Optimizer):\n",
    "    def __init__(self, lr, decay):\n",
    "        # Store the learning rate and decay coefficient\n",
    "        self.lr = lr\n",
    "        self.decay = decay\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, layer_grads, layers, batch_size):\n",
    "        # Loop through the layer grads.  Reverse the layers to match the grads (from output backward to input).\n",
    "        for layer_grad, layer in zip(layer_grads, reversed(layers)):\n",
    "            if layer_grad is None:\n",
    "                # Account for dropout layers\n",
    "                continue\n",
    "            w_grad, b_grad = layer_grad\n",
    "\n",
    "            # Calculate the gradient update size\n",
    "            w_update = -self.lr * w_grad\n",
    "            # Calculate weight decay\n",
    "            w_update -= self.decay * layer.weights\n",
    "\n",
    "            # We don't usually decay the bias\n",
    "            b_update = -self.lr * b_grad\n",
    "\n",
    "            # Actually do the update\n",
    "            layer.update(w_update, b_update)\n",
    "\n",
    "        self.save_vector(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at an example without any weight decay.  If we set the decay coefficient to `0`, it's equivalent to plain SGD.  We'll define a two-layer neural network, then run our training loop.  We'll look at our final weights to check the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Two-layer neural network\n",
    "layers_sgd = [\n",
    "    Dense(7, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "# No decay is equal to SGD\n",
    "sgd = SGDW(1e-3, 0)\n",
    "# Normal SGD\n",
    "training_run(10, \"None\", layers_sgd, sgd, train_data, valid_data, name=\"sgd\")\n",
    "\n",
    "# Plot the final weights\n",
    "sgd.plot_final_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, there are two larger weight values.\n",
    "\n",
    "We can now do the same thing with weight decay.  We'll use a decay coefficient of `1e-3`.  You'll need to experiment with the learning rate and the decay coefficient.  Too large of a decay coefficient relative to the learning rate can prevent the model from learning.  Too small of a decay coefficient won't penalize the large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97dae14e9878469bad338cf593c658b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016756954850279726, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGeCAYAAABGlgGHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAeRklEQVR4nO3dcVTV9f3H8RegXHQJZQSow2FtHTUNEIKDrpXrFmPGjmdruWzBqNypwYbdsxZUwpwl1pLRSZRpkesUabWyms7maOScdEiUnTornTOD4+Kqp8VV2oHi3t8fnd0OP8H4ovCOy/NxzveP++Xz5fu+93Tyeb73e7lhgUAgIAAAACPh1gMAAIDRjRgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGBqjPUAA+H3+/Xvf/9bEyZMUFhYmPU4AABgAAKBgE6cOKHJkycrPPw01z8CDr322muBa6+9NjBp0qSApMALL7ww4GN37doViIiICCQnJzs6Z1tbW0ASGxsbGxsb2wjc2traTvvvvOMrI52dnUpOTtbNN9+s7373uwM+7sMPP1ReXp6uuuoqeb1eR+ecMGGCJKmtrU3R0dGOjgUAADZ8Pp8SExOD/473x3GM5OTkKCcnx/FAt912mxYvXqyIiAht2bLF0bH/e2smOjqaGAEAYIT5vFsshuUG1scff1yHDh1SeXn5gNZ3dXXJ5/P12gAAQGga8hj55z//qZKSEj355JMaM2ZgF2IqKioUExMT3BITE4d4SgAAYGVIY6Snp0eLFy/W8uXLdfHFFw/4uNLSUnV0dAS3tra2IZwSAABYGtKP9p44cUJ79uzRvn37VFRUJOnTj+kGAgGNGTNGf/rTn/TNb37zlONcLpdcLtdQjgYAAL4ghjRGoqOj9eabb/bat3btWr366qt67rnnNG3atKE8PQAAGAEcx8jJkyd18ODB4ON3331XLS0tmjhxoqZOnarS0lIdOXJETzzxhMLDwzVr1qxex8fFxSkqKuqU/QAAYHRyHCN79uzR/Pnzg489Ho8kKT8/Xxs3btT777+v1tbWszchAAAIaWGBQCBgPcTn8fl8iomJUUdHB39nBACAEWKg/37zRXkAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMDelfYAUAYKgklWy1HuEUh1ctsB5hROLKCAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTjmNk586dys3N1eTJkxUWFqYtW7acdv3zzz+vq6++WhdccIGio6OVlZWlV155ZbDzAgCAEOM4Rjo7O5WcnKzq6uoBrd+5c6euvvpqbdu2Tc3NzZo/f75yc3O1b98+x8MCAIDQM8bpATk5OcrJyRnw+qqqql6PV65cqRdffFEvv/yyUlNTnZ4eAACEGMcxcqb8fr9OnDihiRMn9rumq6tLXV1dwcc+n284RgMAAAaG/QbWhx56SCdPntT111/f75qKigrFxMQEt8TExGGcEAAADKdhjZG6ujotX75czzzzjOLi4vpdV1paqo6OjuDW1tY2jFMCAIDhNGxv02zatEm33nqrnn32Wbnd7tOudblccrlcwzQZAACwNCxXRp5++mkVFBTo6aef1oIFC4bjlAAAYIRwfGXk5MmTOnjwYPDxu+++q5aWFk2cOFFTp05VaWmpjhw5oieeeELSp2/N5Ofn6+GHH1ZmZqba29slSePGjVNMTMxZehoAAGCkcnxlZM+ePUpNTQ1+LNfj8Sg1NVVlZWWSpPfff1+tra3B9evXr9cnn3yiwsJCTZo0KbgVFxefpacAAABGMsdXRq688koFAoF+f75x48ZejxsaGpyeAgAAjCJ8Nw0AADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAw5ThGdu7cqdzcXE2ePFlhYWHasmXL5x7T0NCgOXPmyOVy6atf/ao2btw4iFEBAEAochwjnZ2dSk5OVnV19YDWv/vuu1qwYIHmz5+vlpYWLV26VLfeeqteeeUVx8MCAIDQM8bpATk5OcrJyRnw+pqaGk2bNk2rV6+WJM2YMUO7du3Sb37zG2VnZzs9PQAACDGOY8SpxsZGud3uXvuys7O1dOnSfo/p6upSV1dX8LHP5xuq8TDMkkq2Wo9wisOrFliPAACj2pDfwNre3q74+Phe++Lj4+Xz+fTf//63z2MqKioUExMT3BITE4d6TAAAYGTIr4wMRmlpqTweT/Cxz+cjSABgiHDFEtaGPEYSEhLk9Xp77fN6vYqOjta4ceP6PMblcsnlcg31aAAA4AtgyN+mycrKUn19fa99O3bsUFZW1lCfGgAAjACOY+TkyZNqaWlRS0uLpE8/utvS0qLW1lZJn77FkpeXF1x/22236dChQ/rFL36hd955R2vXrtUzzzyjO+644+w8AwAAMKI5jpE9e/YoNTVVqampkiSPx6PU1FSVlZVJkt5///1gmEjStGnTtHXrVu3YsUPJyclavXq1Hn30UT7WCwAAJA3inpErr7xSgUCg35/39ddVr7zySu3bt8/pqQAAwCjAd9MAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMDXGegAAQyepZKv1CKc4vGqB9QgAvmC4MgIAAEwRIwAAwBQxAgAATBEjAADAFDewAvjC4cZbYHThyggAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTfLR3hOKjjwCAUMGVEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYGpQ39pbXV2tX//612pvb1dycrIeeeQRZWRk9Lu+qqpK69atU2trq2JjY3XdddepoqJCUVFRgx4cGE58SzIADB3HMbJ582Z5PB7V1NQoMzNTVVVVys7O1v79+xUXF3fK+rq6OpWUlKi2tlZz587VgQMH9KMf/UhhYWGqrKw8K0/iTPCPDICzhf+fAIPj+G2ayspKLVmyRAUFBZo5c6Zqamo0fvx41dbW9rl+9+7dmjdvnhYvXqykpCRdc801uuGGG9TU1HTGwwMAgJHPUYx0d3erublZbrf7s18QHi63263GxsY+j5k7d66am5uD8XHo0CFt27ZN3/72t/s9T1dXl3w+X68NAACEJkdv0xw/flw9PT2Kj4/vtT8+Pl7vvPNOn8csXrxYx48f19e//nUFAgF98sknuu2223T33Xf3e56KigotX77cyWgAAGCEGvJP0zQ0NGjlypVau3at9u7dq+eff15bt27VihUr+j2mtLRUHR0dwa2trW2oxwQAAEYcXRmJjY1VRESEvF5vr/1er1cJCQl9HrNs2TLddNNNuvXWWyVJs2fPVmdnp3784x/rnnvuUXj4qT3kcrnkcrmcjAYAAEYoRzESGRmptLQ01dfXa+HChZIkv9+v+vp6FRUV9XnMRx99dEpwRERESJICgcAgRgYAYOTiU1encvzRXo/Ho/z8fKWnpysjI0NVVVXq7OxUQUGBJCkvL09TpkxRRUWFJCk3N1eVlZVKTU1VZmamDh48qGXLlik3NzcYJQAAYPRyHCOLFi3SsWPHVFZWpvb2dqWkpGj79u3Bm1pbW1t7XQm59957FRYWpnvvvVdHjhzRBRdcoNzcXN1///1n71kAAIARa1B/gbWoqKjft2UaGhp6n2DMGJWXl6u8vHwwpwIAACGO76YBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApgYVI9XV1UpKSlJUVJQyMzPV1NR02vUffvihCgsLNWnSJLlcLl188cXatm3boAYGAAChZYzTAzZv3iyPx6OamhplZmaqqqpK2dnZ2r9/v+Li4k5Z393drauvvlpxcXF67rnnNGXKFL333ns699xzz8b8AABghHMcI5WVlVqyZIkKCgokSTU1Ndq6datqa2tVUlJyyvra2lp98MEH2r17t8aOHStJSkpKOrOpAQBAyHD0Nk13d7eam5vldrs/+wXh4XK73WpsbOzzmJdeeklZWVkqLCxUfHy8Zs2apZUrV6qnp6ff83R1dcnn8/XaAABAaHIUI8ePH1dPT4/i4+N77Y+Pj1d7e3ufxxw6dEjPPfecenp6tG3bNi1btkyrV6/Wfffd1+95KioqFBMTE9wSExOdjAkAAEaQIf80jd/vV1xcnNavX6+0tDQtWrRI99xzj2pqavo9prS0VB0dHcGtra1tqMcEAABGHN0zEhsbq4iICHm93l77vV6vEhIS+jxm0qRJGjt2rCIiIoL7ZsyYofb2dnV3dysyMvKUY1wul1wul5PRAADACOXoykhkZKTS0tJUX18f3Of3+1VfX6+srKw+j5k3b54OHjwov98f3HfgwAFNmjSpzxABAACji+O3aTwejzZs2KDf/e53evvtt3X77bers7Mz+OmavLw8lZaWBtfffvvt+uCDD1RcXKwDBw5o69atWrlypQoLC8/eswAAACOW44/2Llq0SMeOHVNZWZna29uVkpKi7du3B29qbW1tVXj4Z42TmJioV155RXfccYcuvfRSTZkyRcXFxbrrrrvO3rMAAAAjluMYkaSioiIVFRX1+bOGhoZT9mVlZen1118fzKkAAECI47tpAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgKlBxUh1dbWSkpIUFRWlzMxMNTU1Dei4TZs2KSwsTAsXLhzMaQEAQAhyHCObN2+Wx+NReXm59u7dq+TkZGVnZ+vo0aOnPe7w4cP6+c9/rssvv3zQwwIAgNDjOEYqKyu1ZMkSFRQUaObMmaqpqdH48eNVW1vb7zE9PT268cYbtXz5cl144YVnNDAAAAgtjmKku7tbzc3Ncrvdn/2C8HC53W41Njb2e9yvfvUrxcXF6ZZbbhnQebq6uuTz+XptAAAgNDmKkePHj6unp0fx8fG99sfHx6u9vb3PY3bt2qXHHntMGzZsGPB5KioqFBMTE9wSExOdjAkAAEaQIf00zYkTJ3TTTTdpw4YNio2NHfBxpaWl6ujoCG5tbW1DOCUAALA0xsni2NhYRUREyOv19trv9XqVkJBwyvp//etfOnz4sHJzc4P7/H7/pyceM0b79+/XRRdddMpxLpdLLpfLyWgAAGCEcnRlJDIyUmlpaaqvrw/u8/v9qq+vV1ZW1inrp0+frjfffFMtLS3B7Tvf+Y7mz5+vlpYW3n4BAADOroxIksfjUX5+vtLT05WRkaGqqip1dnaqoKBAkpSXl6cpU6aooqJCUVFRmjVrVq/jzz33XEk6ZT8AABidHMfIokWLdOzYMZWVlam9vV0pKSnavn178KbW1tZWhYfzh10BAMDAOI4RSSoqKlJRUVGfP2toaDjtsRs3bhzMKQEAQIjiEgYAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwNagYqa6uVlJSkqKiopSZmammpqZ+127YsEGXX365zjvvPJ133nlyu92nXQ8AAEYXxzGyefNmeTwelZeXa+/evUpOTlZ2draOHj3a5/qGhgbdcMMN+stf/qLGxkYlJibqmmuu0ZEjR854eAAAMPI5jpHKykotWbJEBQUFmjlzpmpqajR+/HjV1tb2uf6pp57ST37yE6WkpGj69Ol69NFH5ff7VV9ff8bDAwCAkc9RjHR3d6u5uVlut/uzXxAeLrfbrcbGxgH9jo8++kgff/yxJk6c2O+arq4u+Xy+XhsAAAhNjmLk+PHj6unpUXx8fK/98fHxam9vH9DvuOuuuzR58uReQfP/VVRUKCYmJrglJiY6GRMAAIwgw/ppmlWrVmnTpk164YUXFBUV1e+60tJSdXR0BLe2trZhnBIAAAynMU4Wx8bGKiIiQl6vt9d+r9erhISE0x770EMPadWqVfrzn/+sSy+99LRrXS6XXC6Xk9EAAMAI5ejKSGRkpNLS0nrdfPq/m1GzsrL6Pe7BBx/UihUrtH37dqWnpw9+WgAAEHIcXRmRJI/Ho/z8fKWnpysjI0NVVVXq7OxUQUGBJCkvL09TpkxRRUWFJOmBBx5QWVmZ6urqlJSUFLy35JxzztE555xzFp8KAAAYiRzHyKJFi3Ts2DGVlZWpvb1dKSkp2r59e/Cm1tbWVoWHf3bBZd26deru7tZ1113X6/eUl5frl7/85ZlNDwAARjzHMSJJRUVFKioq6vNnDQ0NvR4fPnx4MKcAAACjBN9NAwAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwNKkaqq6uVlJSkqKgoZWZmqqmp6bTrn332WU2fPl1RUVGaPXu2tm3bNqhhAQBA6HEcI5s3b5bH41F5ebn27t2r5ORkZWdn6+jRo32u3717t2644Qbdcsst2rdvnxYuXKiFCxfqrbfeOuPhAQDAyOc4RiorK7VkyRIVFBRo5syZqqmp0fjx41VbW9vn+ocffljf+ta3dOedd2rGjBlasWKF5syZozVr1pzx8AAAYOQb42Rxd3e3mpubVVpaGtwXHh4ut9utxsbGPo9pbGyUx+PptS87O1tbtmzp9zxdXV3q6uoKPu7o6JAk+Xw+J+MOiL/ro7P+O8/UQJ4nc589zD28mHt4MffwCuW5z+T3BgKB0y8MOHDkyJGApMDu3bt77b/zzjsDGRkZfR4zduzYQF1dXa991dXVgbi4uH7PU15eHpDExsbGxsbGFgJbW1vbafvC0ZWR4VJaWtrraorf79cHH3yg888/X2FhYYaT9c/n8ykxMVFtbW2Kjo62Hifk8XoPL17v4cXrPbx4vYdOIBDQiRMnNHny5NOucxQjsbGxioiIkNfr7bXf6/UqISGhz2MSEhIcrZckl8sll8vVa9+5557rZFQz0dHR/Mc8jHi9hxev9/Di9R5evN5DIyYm5nPXOLqBNTIyUmlpaaqvrw/u8/v9qq+vV1ZWVp/HZGVl9VovSTt27Oh3PQAAGF0cv03j8XiUn5+v9PR0ZWRkqKqqSp2dnSooKJAk5eXlacqUKaqoqJAkFRcX64orrtDq1au1YMECbdq0SXv27NH69evP7jMBAAAjkuMYWbRokY4dO6aysjK1t7crJSVF27dvV3x8vCSptbVV4eGfXXCZO3eu6urqdO+99+ruu+/W1772NW3ZskWzZs06e8/iC8Dlcqm8vPyUt5cwNHi9hxev9/Di9R5evN72wgKBz/u8DQAAwNDhu2kAAIApYgQAAJgiRgAAgCliBAAAmCJGzoLq6molJSUpKipKmZmZampqsh4pJFVUVOiyyy7ThAkTFBcXp4ULF2r//v3WY40aq1atUlhYmJYuXWo9Ssg6cuSIfvjDH+r888/XuHHjNHv2bO3Zs8d6rJDU09OjZcuWadq0aRo3bpwuuugirVix4vO/QwVDghg5Q5s3b5bH41F5ebn27t2r5ORkZWdn6+jRo9ajhZzXXntNhYWFev3117Vjxw59/PHHuuaaa9TZ2Wk9Wsh744039Nvf/laXXnqp9Sgh6z//+Y/mzZunsWPH6o9//KP+8Y9/aPXq1TrvvPOsRwtJDzzwgNatW6c1a9bo7bff1gMPPKAHH3xQjzzyiPVooxIf7T1DmZmZuuyyy7RmzRpJn/5F2sTERP30pz9VSUmJ8XSh7dixY4qLi9Nrr72mb3zjG9bjhKyTJ09qzpw5Wrt2re677z6lpKSoqqrKeqyQU1JSor/97W/661//aj3KqHDttdcqPj5ejz32WHDf9773PY0bN05PPvmk4WSjE1dGzkB3d7eam5vldruD+8LDw+V2u9XY2Gg42ejQ0dEhSZo4caLxJKGtsLBQCxYs6PXfOc6+l156Senp6fr+97+vuLg4paamasOGDdZjhay5c+eqvr5eBw4ckCT9/e9/165du5STk2M82ej0hfzW3pHi+PHj6unpCf712f+Jj4/XO++8YzTV6OD3+7V06VLNmzcv5P6a7xfJpk2btHfvXr3xxhvWo4S8Q4cOad26dfJ4PLr77rv1xhtv6Gc/+5kiIyOVn59vPV7IKSkpkc/n0/Tp0xUREaGenh7df//9uvHGG61HG5WIEYxIhYWFeuutt7Rr1y7rUUJWW1ubiouLtWPHDkVFRVmPE/L8fr/S09O1cuVKSVJqaqreeust1dTUECND4JlnntFTTz2luro6XXLJJWppadHSpUs1efJkXm8DxMgZiI2NVUREhLxeb6/9Xq9XCQkJRlOFvqKiIv3hD3/Qzp079eUvf9l6nJDV3Nyso0ePas6cOcF9PT092rlzp9asWaOuri5FREQYThhaJk2apJkzZ/baN2PGDP3+9783mii03XnnnSopKdEPfvADSdLs2bP13nvvqaKighgxwD0jZyAyMlJpaWmqr68P7vP7/aqvr1dWVpbhZKEpEAioqKhIL7zwgl599VVNmzbNeqSQdtVVV+nNN99US0tLcEtPT9eNN96olpYWQuQsmzdv3ikfVT9w4IC+8pWvGE0U2j766KNeX+oqSREREfL7/UYTjW5cGTlDHo9H+fn5Sk9PV0ZGhqqqqtTZ2amCggLr0UJOYWGh6urq9OKLL2rChAlqb2+XJMXExGjcuHHG04WeCRMmnHI/zpe+9CWdf/753KczBO644w7NnTtXK1eu1PXXX6+mpiatX79e69evtx4tJOXm5ur+++/X1KlTdckll2jfvn2qrKzUzTffbD3a6BTAGXvkkUcCU6dODURGRgYyMjICr7/+uvVIIUlSn9vjjz9uPdqoccUVVwSKi4utxwhZL7/8cmDWrFkBl8sVmD59emD9+vXWI4Usn88XKC4uDkydOjUQFRUVuPDCCwP33HNPoKury3q0UYm/MwIAAExxzwgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABT/wcg8huy3NUWXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_sgdw = [\n",
    "    Dense(7, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "# No decay is equal to SGD\n",
    "sgd = SGDW(1e-3, 5e-4)\n",
    "# Weight decay\n",
    "training_run(10, \"Weight Decay\", layers_sgdw, sgd, train_data, valid_data, name=\"sgdw\")\n",
    "sgd.plot_final_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our weight values are closer together, and the largest values are now smaller.  This can help the network generalize better to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 28.325657408857865\n",
      "SGDW: 107.32555507049128\n"
     ]
    }
   ],
   "source": [
    "print(f\"SGD: {test_loss(layers_sgd, test_data)}\")\n",
    "print(f\"SGDW: {test_loss(layers_sgdw, test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, our test set loss is higher with weight decay than without it.  Weight decay won't always reduce test set loss - it depends on how similar the training and test set are, as well as other characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout is a regularization technique that randomly sets some activations in the network to zero.  We add in dropout right after the activation function:\n",
    "\n",
    "![](images/regularization/network_dropout.svg)\n",
    "\n",
    "A dropout layer will randomly set some inputs to zero.  The layer takes one parameter, which is the probability that any single value will be set to `0`.  This is usually called `p` or `drop_p`.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "![](images/regularization/dropout.svg)\n",
    "\n",
    "In this example, the dropout layer sets a single activation to 0, based on the probability of `.5`.\n",
    "\n",
    "You might be wondering what the point of this is - it seems like it just throws away valuable inputs.  Dropout forces the network to make predictions with a subset of the full network.  This means that the network is more robust to noisy input or other issues. It can even be thought of as a form of ensembling, where multiple smaller models are combined into a larger, more powerful one.\n",
    "\n",
    "Here's another look at the same operations from before:\n",
    "\n",
    "![](images/regularization/ensemble_network.svg)\n",
    "\n",
    "As you can see, in the example with dropout, predictions are made by a subset of the network.  If we repeatedly apply dropout, we force the network to become more resilient, and minimize overfitting.\n",
    "\n",
    "Just as important as when we apply dropout is when we don't apply it.  We don't apply dropout after the final output layer. This will result in many of our predictions being zero, which isn't what we want.  We also don't use dropout directly on the inputs. We basically only want to use dropout inside the network, to force the network to use a subset to make the predictions.\n",
    "\n",
    "We also only use dropout in training, not in inference.  In inference, using dropout will result in lower accuracy without any gains (the gain is in training the network to be more resilient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a dropout layer that we can then use in our network.  The layer will be relatively straightforward:\n",
    "\n",
    "- We'll set a `drop_p`, which is the probability that we'll drop out (set to 0) any single value.\n",
    "- In the forward pass, we'll randomly set some values to 0.\n",
    "- In the backward pass, we'll set the gradient to zero where we applied dropout.  Similarly to Relu, when we zero out a value, it doesn't contribute to the prediction (and loss), so we don't want to use it in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    def __init__(self, drop_p):\n",
    "        # Probability that we'll drop out any single input\n",
    "        self.drop_p = drop_p\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            # Generate a mask of 0s and 1s, using the drop probability\n",
    "            self.mask = np.random.binomial(1, 1-self.drop_p, input.shape)\n",
    "        else:\n",
    "            # No dropout in inference\n",
    "            self.mask = np.ones_like(input)\n",
    "        # Apply the mask.  If the mask is 0, the input is set to 0\n",
    "        return np.where(self.mask, input, 0)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Use np.where to zero out the gradient where we did dropout\n",
    "        return None, np.where(self.mask, grad, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6daba121f944f3ba5fee71a1b553a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016758120833158804, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_dropout = [\n",
    "    Dense(7, 10),\n",
    "    Dropout(.2),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(1e-3, 5e-4)\n",
    "# Weight decay and dropout\n",
    "training_run(10, \"Weight Decay + Dropout\", layers_dropout, sgd, train_data, valid_data, name=\"dropout\")\n",
    "sgd.plot_final_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDW: 107.32555507049128\n",
      "SGDW + Dropout: 13.244365053084687\n"
     ]
    }
   ],
   "source": [
    "print(f\"SGDW: {test_loss(layers_sgdw, test_data)}\")\n",
    "print(f\"SGDW + Dropout: {test_loss(layers_dropout, test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, combining SGDW and dropout worked effectively to prevent overfitting.  I would encourage you to play around with the drop probability, and look at the loss in the W&B dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "Early stopping can prevent overfitting by stopping training when the validation loss is plateauing or increasing.  It's the simplest form of regularization, but can also be the most powerful.\n",
    "\n",
    "The straightforward way to do early stopping is to watch the validation loss, then stop training as soon as you observe it increasing.  You may need to \"rewind\" an epoch or two to get back to before the loss started to increase.  It's common to save checkpoints regularly while training, and then choose the best one after observing the loss.\n",
    "\n",
    "![](images/regularization/early_stopping.png)\n",
    "\n",
    "In the example, above, we'd want to stop training after epoch 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66612dff3fb04446a7a3a08febd57c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016775147216200516, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_early = [\n",
    "    Dense(7, 10),\n",
    "    Dropout(.2),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(1e-3, 5e-4)\n",
    "# Weight decay and dropout\n",
    "training_run(4, \"Early Stopping\", layers_early, sgd, train_data, valid_data, name=\"early_stopping\")\n",
    "sgd.plot_final_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDW + Dropout: 13.244365053084687\n",
      "SGDW + Dropout + Early Stopping: 15.488170569363843\n"
     ]
    }
   ],
   "source": [
    "print(f\"SGDW + Dropout: {test_loss(layers_dropout, test_data)}\")\n",
    "print(f\"SGDW + Dropout + Early Stopping: {test_loss(layers_early, test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, adding in early stopping doesn't improve test set accuracy, but it does get similar accuracy with less than half the epochs.  When you're training larger models, a single epoch can take days or weeks, and saving time is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training convergence\n",
    "\n",
    "We'll now explore some techniques that aren't strictly regularization, but can help with training stability, convergence, and overfitting.\n",
    "\n",
    "When you're training deep neural networks with many layers, it's common for gradients to vanish/explode (get very small or very large) due to the sheer number of matrix multiplications.  If you multiply a number by another number between `0` and `1` repeatedly, it will get very small.  If you multiply a number by another number greater than `1` repeatedly, it will get very large.\n",
    "\n",
    "We discussed this in a slightly different context in the lesson on RNNs.  This can cause the network to become unstable - loss will increase or decrease wildly from epoch to epoch.  Ideally, we'd have a nice smoothly descending loss curve, like this one:\n",
    "\n",
    "![](images/regularization/smooth_descent.png)\n",
    "\n",
    "Unstable loss curves, with spikes, can result in training where the loss ends up increasing (training never converges), or bad predictions.  Here's an example of loss spikes in training:\n",
    "\n",
    "![](images/regularization/loss_spikes.png)\n",
    "\n",
    "\n",
    "A smoothly descending loss curve with no spikes is one that is stable and converging well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual connections\n",
    "\n",
    "One technique that can help with stable convergence is called residual connection.  Residual connections add the output of the last layer to the output of the current layer.  This helps gradients \"flow\" through large networks better.\n",
    "\n",
    "![](images/regularization/residual_gradient.svg)\n",
    "\n",
    "As you can see, the backward gradient flows from the output to both the previous layer, and the layer before that.  This can help with the vanishing gradient problem (because the residual flow is not altered by any operations).\n",
    "\n",
    " Read more about residual connections [here](https://arxiv.org/pdf/1512.03385v1.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll create a layer that has a residual connection.  It will add in the output from the previous layer to the current layer output.  It will do something similar in the backward pass by summing the incoming gradient with the output gradient.\n",
    "\n",
    "Note that in order to use a residual connection, the gradients for both layers need to be the same size.  This means that the output of the layers should have the same shape (same number of columns).  We'll add another layer, so we can have two layers with the same output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseResidualConnection(Dense):\n",
    "    \"\"\"\n",
    "    Dense layer with a residual connection\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        output = super().forward(x)\n",
    "        return x + output # Sum input with output\n",
    "\n",
    "    def backward(self, grad):\n",
    "        param_grads, out_grad = super().backward(grad)\n",
    "        # Sum incoming gradient with calculated gradient\n",
    "        return param_grads, grad + out_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589901bd938143c5b4773a63177052ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016756215967082728, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_resid = [\n",
    "    Dense(7, 10),\n",
    "    DenseResidualConnection(10, 10),\n",
    "    Dropout(.2),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(1e-4, 5e-4)\n",
    "# Weight decay and dropout\n",
    "training_run(10, \"Residual Connection\", layers_resid, sgd, train_data, valid_data, name=\"residual_connection\")\n",
    "sgd.plot_final_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDW + Dropout + Early Stopping: 15.488170569363843\n",
      "Residual Connection: 26.43739286149877\n"
     ]
    }
   ],
   "source": [
    "print(f\"SGDW + Dropout + Early Stopping: {test_loss(layers_early, test_data)}\")\n",
    "print(f\"Residual Connection: {test_loss(layers_resid, test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the issues with residual connections is that they increase the size of the activations.  With many layers (a deep network can have hundreds of layers), we'll essentially double the activations each time we have a residual connection.  This can quickly make the values explode.\n",
    "\n",
    "We can fix this by using a technique called layer normalization.  Residual connections and layer normalization are often used together, including in transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layernorm\n",
    "\n",
    "Layer normalization (or layernorm) will normalize the values in a layer by subtracting the mean and dividing by standard deviation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}_i = \\frac{\\mathbf{x}_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}} * \\gamma + \\beta\n",
    "\\end{equation}\n",
    "\n",
    "The mean and standard deviation are calculated for each row of data.  The $\\gamma$ and $\\beta$ parameters scale the data, similarly to weights and biases.  The goal of these parameters is to ensure that the model can learn the optimal range for the values.  However, using them is becoming less common, as subsequent layers can rescale the values.  We'll leave off $\\gamma$ and $\\beta$ in our implementation.\n",
    "\n",
    "The purpose of layernorm is to ensure that values stay in a tighter range around 0 during training.  This enables using higher learning rates, and reduces the chance of loss spikes and other training instability.\n",
    "\n",
    "If you haven't looked at the previous lesson on the [computational graph](https://github.com/VikParuchuri/zero_to_gpt/blob/master/explanations/comp_graph.ipynb), I recommend doing so before looking at the LayerNorm implementation.  It will make understanding the backward pass of the LayerNorm much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the computational graph for the forward pass of LayerNorm:\n",
    "\n",
    "![](images/regularization/layernorm_steps_full.svg)\n",
    "\n",
    "We'll implement this in code, then reverse it to get the backward pass.  There's a lot going on in this graph.  It's much more complex than you might think, so I'd recommend looking at the computational graph lesson, and working through the backward pass yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm():\n",
    "    def __init__(self, embed_dim, eps):\n",
    "        self.embed_dim = embed_dim\n",
    "        # eps is a parameter added for numerical stability when taking a square root\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Cache for backward pass\n",
    "        self.input = input\n",
    "        # Calculate the mean and standard deviation\n",
    "        self.mean = np.sum(input, axis=1, keepdims=True) / self.embed_dim\n",
    "        self.normed = (input - self.mean)\n",
    "        variance = np.sum(self.normed**2, axis=1, keepdims=True) / self.embed_dim\n",
    "        self.std = np.sqrt(variance + self.eps)\n",
    "        inverse_std = 1 / self.std\n",
    "        # Normalize the input\n",
    "        self.output = self.normed * inverse_std\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad):\n",
    "        \"\"\"\n",
    "        This is a long function, but it is just \"undoing\" the forward pass\n",
    "        \"\"\"\n",
    "        # Find the derivative of numerator (normed)\n",
    "        # Grad times inverse standard deviation\n",
    "        grad_normed_1 = grad * 1 / self.std\n",
    "\n",
    "        # Derivative of denominator (std)\n",
    "        grad_std = grad * self.normed\n",
    "        # std is a single number\n",
    "        grad_std = np.sum(grad_std, axis=1, keepdims=True)\n",
    "        # Derivative of 1 / std\n",
    "        grad_std = grad_std * -1 / (self.std**2)\n",
    "\n",
    "        # Find gradient against the variance\n",
    "        grad_variance = grad_std * .5 * 1 / self.std\n",
    "\n",
    "        # Find gradient against normed\n",
    "        grad_normed_2 = grad_variance * 1 / self.embed_dim\n",
    "        grad_normed_2 = np.ones_like(self.normed, dtype=self.input.dtype) * grad_normed_2\n",
    "        grad_normed_2 = grad_normed_2 * 2 * self.normed\n",
    "\n",
    "        # Combine two gradients against normed\n",
    "        grad_normed = grad_normed_1 + grad_normed_2\n",
    "\n",
    "        # Find gradient against mean\n",
    "        grad_mean = grad_normed * -1\n",
    "        grad_mean = np.sum(grad_mean, axis=1, keepdims=True)\n",
    "\n",
    "        # Find gradient against input\n",
    "        grad_input_1 = grad_normed\n",
    "        grad_input_2 =  grad_mean * 1 / self.embed_dim\n",
    "        grad_input_2 = grad_input_2 * np.ones_like(self.input, dtype=self.input.dtype)\n",
    "\n",
    "        # Combine two gradients against input\n",
    "        grad_input = grad_input_1 + grad_input_2\n",
    "        return None, grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comparison, we can implement a version of the backward pass that comes from a derivation of the LayerNorm formula.  Reversing the computational graph takes more code, but is easier to trace and understand if you don't have a math background.  Both methods should give us the same answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormAnalytic(LayerNorm):\n",
    "    def backward(self, grad):\n",
    "        y_sum = np.sum(grad, axis=1, keepdims=True)\n",
    "        N = 1 / self.embed_dim\n",
    "        c1 = N * np.sum(self.output * grad, axis=1, keepdims=True)\n",
    "        c2 = N * y_sum\n",
    "        return None, 1/self.std * (grad - c1 * self.output - c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test our layernorm implementation to see if it improves convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6b118b40c4488c84431807d22d617d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016753906249262703, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_ln = [\n",
    "    Dense(7, 10),\n",
    "    DenseResidualConnection(10, 10),\n",
    "    LayerNorm(10, 1e-6),\n",
    "    Dropout(.2),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(1e-3, 1e-4)\n",
    "# Weight decay and dropout\n",
    "training_run(25, \"Layer Norm\", layers_ln, sgd, train_data, valid_data, name=\"layer_norm\")\n",
    "sgd.plot_final_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDW + Residual Connection: 26.43739286149877\n",
      "SGDW + Residual Connection + LayerNorm: 433.9127938623826\n"
     ]
    }
   ],
   "source": [
    "print(f\"SGDW + Residual Connection: {test_loss(layers_resid, test_data)}\")\n",
    "print(f\"SGDW + Residual Connection + LayerNorm: {test_loss(layers_ln, test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the W&B dashboard, you should see that the loss curve appears smoother now:\n",
    "\n",
    "![](images/regularization/layer_norm.png)\n",
    "\n",
    "Of course, the test set loss is much higher with layer normalization than without it.  We could experiment with different learning rates and other parameters to try to fix this.  For example, a learning rate of `1e-3` results in lower training loss than other techniques:\n",
    "\n",
    "![](images/regularization/layer_norm_high_lr.png)\n",
    "\n",
    "Layer normalization enables us to use a higher learning rate, especially when combined with residual connections.  This can make training convergence faster, but is not guaranteed to increase test and validation set accuracy.  It can take a lot of experimentation to find the right set of layers and parameters to optimize test set accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "We've covered several building blocks that will help you reduce overfitting and improve convergence in deep neural networks.  I'd recommend experimenting with these techniques while adding in more layers to see what they do in a very deep network.\n",
    "\n",
    "We now have most of the building blocks necessary to create a transformer layer.  Transformers are the models that have powered many recent advanced in AI, like LLMs.  In the next couple of lessons, we'll learn about PyTorch and how to work with text in PyTorch.  Then, we'll replicate the original transformers paper!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
