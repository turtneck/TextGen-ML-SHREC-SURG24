{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with text\n",
    "\n",
    "In this lesson, we'll learn how to work with text input to neural networks.  This is necessary to build a language model like GPT.\n",
    "\n",
    "Neural networks can't understand text directly, so we need to convert the text into a numerical representation.  How we choose to represent the text can give the model a lot of useful information, and result in better predictions.\n",
    "\n",
    "The steps in converting text to a numerical representation are:\n",
    "\n",
    "1. Tokenize the text to convert it into discrete tokens (kind of like words)\n",
    "2. Assign a number to each token\n",
    "3. Convert each token id into a vector representation.  This is called embedding.\n",
    "\n",
    "We can then feed the vectors into a neural network layer.  Here's a diagram:\n",
    "\n",
    "![](images/text/text_to_vec.svg)\n",
    "\n",
    "You might be wondering why we don't directly feed the token ids into the neural network.\n",
    "\n",
    "Embedding enables the network to learn similarities between tokens.  For example, the token id for a `.` might be `2`, and the id for a ` ` might be `7`.  This doesn't help the network understand the relationship between the two tokens.  However, if the vector for a `.` is `[0,1,.1,2]`, and the vector for a ` ` is `[0,1,.1,1]`, the distance between the vectors could indicate that the tokens are similar in their function.  Like weights, the embeddings are learned by the network, and will change during training.  Tokens that are conceptually similar will have vectors that are closer together than tokens that aren't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the data\n",
    "\n",
    "We'll be working with a dataset from [Opus books](https://huggingface.co/datasets/opus_books/viewer/en-es/train).  This dataset contains English sentences from books, and their Spanish translations.  We'll use the translation in the next lesson, but in this one, we'll only use the English sentence.\n",
    "\n",
    "There are about 24k sentence pairs in the dataset.  Here's an example:\n",
    "\n",
    "![](images/text/sentences.svg)\n",
    "\n",
    "These sentences in very Old(e) English, but that won't stop our AI from parsing them.  We'll first load in the data using `pandas` and explore it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the society of his nephew and niece, and th...</td>\n",
       "      <td>En compañía de su sobrino y sobrina, y de los ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By a former marriage, Mr. Henry Dashwood had o...</td>\n",
       "      <td>De un matrimonio anterior, el señor Henry Dash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>By his own marriage, likewise, which happened ...</td>\n",
       "      <td>Además, su propio matrimonio, ocurrido poco de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But the fortune, which had been so tardy in co...</td>\n",
       "      <td>Pero la fortuna, que había tardado tanto en ll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But Mrs. John Dashwood was a strong caricature...</td>\n",
       "      <td>Pero la señora de John Dashwood era una áspera...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  In the society of his nephew and niece, and th...   \n",
       "1  By a former marriage, Mr. Henry Dashwood had o...   \n",
       "2  By his own marriage, likewise, which happened ...   \n",
       "3  But the fortune, which had been so tardy in co...   \n",
       "4  But Mrs. John Dashwood was a strong caricature...   \n",
       "\n",
       "                                                  es  \n",
       "0  En compañía de su sobrino y sobrina, y de los ...  \n",
       "1  De un matrimonio anterior, el señor Henry Dash...  \n",
       "2  Además, su propio matrimonio, ocurrido poco de...  \n",
       "3  Pero la fortuna, que había tardado tanto en ll...  \n",
       "4  Pero la señora de John Dashwood era una áspera...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This file is in the repo if you clone it\n",
    "opus = pd.read_csv(\"../data/opus_books.csv\")\n",
    "opus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our vocabulary\n",
    "\n",
    "Now, we need to clean the data and define our token vocabulary.  Our vocabulary is how we map each token to a unique token id.  We'll be creating our own very simple tokenizer and vocabulary.  In practice, you'll use more powerful tokenizers like byte-pair encoding that look at sequences of characters to find the optimal tokenization scheme.\n",
    "\n",
    "Optimal means accurate and fast.  For example, we could look at individual characters (`a`, `b`, etc) instead of tokens.  This would result in a much smaller vocabulary (and run faster), but it would be much less accurate, since the model would get less information about entire words and concepts.\n",
    "\n",
    "We'll first setup some special tokens, that the system will use:\n",
    "\n",
    "- `<PAD>` - this token is used to pad sequences to a given length.  When we're working with text data, sentences won't all be the same length.  However, a neural network needs all rows in a batch to have the same number of columns.  Padding enables us to make all sentences the same length.  We use a special token for this, and tell the network to ignore it in the backward pass.\n",
    "- `<UNK>` - some tokens don't occur often enough to add them to our vocabulary.  Imagine words like `Octothorpe`, or issues with data quality like `hello123bye`.  These long-tail words will add a lot to our vocabulary (and make our model slower), but don't add much value to the model.  More powerful tokenizers will split these up into individual characters, but in our simple tokenizer, we need `UNK`.\n",
    "- `<BOS>` - this special token is used to mark the beginning of a sentence, or a sequence.\n",
    "- `<EOS>` - used to mark the end of a sequence.  It helps the network understand when to stop generating text.\n",
    "\n",
    "Some tokenizers, like the GPT-2 tokenizer, don't have `BOS` and `EOS`, and use `PAD` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "special_tokens = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"<UNK>\": 1,\n",
    "    \"<BOS>\": 2,\n",
    "    \"<EOS>\": 3\n",
    "}\n",
    "vocab = special_tokens.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our functions to clean and tokenize input text.  We're going to do some naive cleaning, and just strip anything that isn't in a small set of characters (letters, numbers, spaces, some punctuation).  We're doing this because our simple tokenizer needs a very small character set (a large character set will result in a larger vocabulary).  As you'll see later, the size of the vocabulary impacts the size of the embedding matrix, and thus the performance of the network.\n",
    "\n",
    "Our tokenization will just split on whitespace and punctuation.  We'll set a limit on how many tokens we want per sentence for performance reasons.  Any sentences that are shorter will be padded on the left with the `<PAD>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the maximum numbers of tokens we'll keep from each sentence.  You can increase this, but training will take longer.\n",
    "token_limit = 11\n",
    "\n",
    "def clean(text):\n",
    "    # Use re to replace punctuation that is not a comma, question mark, or period with spaces\n",
    "    text = re.sub(r'[^\\w\\s,?.!]',' ', text)\n",
    "    # Strip leading/trailing space\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    # Split on consecutive whitespace and punctuation\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]+|[\\s]+', text)\n",
    "\n",
    "    # Pad sentences that are too short\n",
    "    if len(tokens) < token_limit:\n",
    "        pad_count = token_limit - len(tokens)\n",
    "        tokens = [\"<PAD>\"] * pad_count + tokens\n",
    "\n",
    "    # Only take tokens up to the limit\n",
    "    tokens = tokens[:token_limit]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " 'This',\n",
       " ' ',\n",
       " 'sentence',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'okay',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example tokenization\n",
    "tokenize(\"This sentence is okay.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a vocabulary using our functions.  We'll first create a dictionary containing every token in our sentences, and the number of times it appears across the dataset.  Then, we'll create a vocab dictionary, only selecting the tokens that appear more than once.  Tokens that only appear once will be marked as unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opus_tokens = defaultdict(int)\n",
    "\n",
    "# Loop through the sentences, clean, tokenize, and assign token counts\n",
    "for index, row in opus.iterrows():\n",
    "    cleaned = clean(row[\"en\"])\n",
    "    tokens = tokenize(cleaned)\n",
    "    for token in tokens:\n",
    "        opus_tokens[token] += 1\n",
    "\n",
    "# Set to the current size of the vocabulary (special tokens)\n",
    "counter = len(vocab)\n",
    "# Assign a unique id to each token if it appears more than once\n",
    "for index, token in enumerate(opus_tokens):\n",
    "    # Filter out uncommon tokens\n",
    "    # Add unknown token for rare words\n",
    "    if opus_tokens[token] > 1:\n",
    "        vocab[token] = counter\n",
    "        counter += 1\n",
    "    else:\n",
    "        vocab[token] = 1 # Assign unknown id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11731"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 11k tokens in our vocabulary.  In practice, tokenizers will usually have between 10k and 100k tokens.  This is a good tradeoff between thoroughness (having a unique id for every word), and vocabulary size (splitting some rare words into multiple tokens).  The GPT-2 tokenizer uses 50257 tokens.\n",
    "\n",
    "We'll also build a reverse vocab lookup, which we can use to decode token ids to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Several tokens could be mapped to the <UNK> token id, so make sure we set the reverse mapping correctly\n",
    "for k, v in special_tokens.items():\n",
    "    reverse_vocab[v] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences\n",
    "\n",
    "We can now use our vocabulary to tokenize our sentences.  We'll create an encode function, that can turn a sentence into a torch tensor of token ids.\n",
    "\n",
    "We'll also write a decode function.  This will use a reverse lookup to go from token id to token.  This will enable us to decode our predictions and see how good they were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def encode(text):\n",
    "    # Yokenize input text\n",
    "    tokens = tokenize(clean(text))\n",
    "    # Convert to token ids\n",
    "    encoded = torch.tensor([vocab[token] for token in tokens])\n",
    "    return encoded\n",
    "\n",
    "def decode(encoded):\n",
    "    # The input is a torch tensor - convert it to a list\n",
    "    encoded = encoded.detach().cpu().tolist()\n",
    "    # Decode a list of integers into text\n",
    "    decoded = \"\".join([reverse_vocab[token] for token in encoded])\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the encode function to convert our English sentences into token ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for index, row in opus.iterrows():\n",
    "    # Encode the English sentences\n",
    "    en_text = row[\"en\"]\n",
    "    en = encode(en_text)\n",
    "    tokenized.append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  5,  6,  5,  7,  5,  8,  5,  9,  5, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch dataset\n",
    "\n",
    "Once we have our encoded vectors, we'll need to create a torch dataset with the input tokens (first 10 tokens of each sentence), and the token we want to predict (token 11).\n",
    "\n",
    "This is similar to what we did in the [last lesson](https://github.com/VikParuchuri/zero_to_gpt/blob/master/explanations/pytorch.ipynb) when we created a dataset to use in training.\n",
    "\n",
    "We'll also create a DataLoader, which will enable us to batch our data for better performance.  If multiple sentences are batched together, the whole batch will be processed at once, versus serially.  The tradeoff is higher memory usage (the whole batch has to fit into memory at once, as do the intermediate values/gradients).  But this data is small enough that it won't matter if we use a high batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextData(Dataset):\n",
    "    \"\"\"\n",
    "    A torch dataset that stores encoded text data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        # The input is a list of torch tensors.  We need to stack them into a 2-D tensor.\n",
    "        self.tokens = torch.vstack(data).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return how many examples are in the dataset\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a single training example\n",
    "        x = self.tokens[idx][:10]\n",
    "        y = self.tokens[idx][10]\n",
    "        return x, y\n",
    "\n",
    "# Initialize the dataset\n",
    "train_ds = TextData(tokenized)\n",
    "# Initialize dataloader with a high batch size\n",
    "train = DataLoader(train_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 5, 6, 5, 7, 5, 8, 5, 9, 5]), tensor(10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first element of the dataset\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 4,  5,  6,  5,  7,  5,  8,  5,  9,  5],\n",
       "         [11,  5, 12,  5, 13,  5, 14, 15,  5, 16],\n",
       "         [11,  5,  9,  5, 18,  5, 14, 15,  5, 19],\n",
       "         [20,  5,  6,  5, 21, 15,  5, 22,  5, 23],\n",
       "         [20,  5, 24, 17,  5, 25,  5, 26,  5, 27],\n",
       "         [28,  5, 29,  5, 30,  5, 31, 15,  5, 32],\n",
       "         [33,  5, 27,  5, 34,  5, 35,  5, 36, 37],\n",
       "         [33,  5, 27,  5,  1, 15,  5, 39, 15,  5],\n",
       "         [41,  5, 42, 15,  5, 43,  5, 44, 15,  5],\n",
       "         [45,  5, 46,  5, 47,  5, 48,  5, 49,  5],\n",
       "         [50,  5, 51,  5,  8,  5, 52,  5, 22,  5],\n",
       "         [41, 15,  5, 54, 15,  5, 27,  5, 55,  5],\n",
       "         [57,  5,  1,  5, 32,  5, 12,  5, 58,  5],\n",
       "         [24, 17,  5, 25,  5, 26,  5, 60,  5, 61],\n",
       "         [62,  5, 63,  5, 64,  5, 65,  5, 66,  5],\n",
       "         [68,  5, 69,  5, 70,  5, 71,  5, 72,  5]]),\n",
       " tensor([10, 17, 15,  5,  5,  5, 38, 40,  6, 32, 53, 56, 59,  5, 67, 73])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataloader is an iterator\n",
    "# next(iter()) will get the first batch\n",
    "batch = next(iter(train))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the DataLoader automatically batches our data together.  The input tokens are 2-dimensional with the shape `(B, T)` where `B` is the size of the batch, and `T` is the number of tokens in each input sentence.  Our prediction target is one-dimensional, with shape `B`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our network\n",
    "\n",
    "We now have a sequence of token ids for each sentence.  In order to train a network to predict the next token, we first need to embed each token into a vector representation.\n",
    "\n",
    "### Embedding layer\n",
    "\n",
    "We can use an embedding layer for this.  An embedding layer works like this:\n",
    "\n",
    "- Define an embedding size.  This is the length of the embedding vector for each token.  This is similar to the number of predictor columns in earlier lessons.  Think of each item in the embedding vector as a predictor the network can use.  The higher the embedding size, the more nuance the network can pick up in each token, at the cost of higher memory usage and slower performance.\n",
    "- Create a matrix of size (vocab_size, embedding_size) and randomly initialize it.  This will create a separate unique embedding vector for each token id.\n",
    "- In the forward pass, index the matrix to lookup the vector associated with the token id.\n",
    "\n",
    "![](images/text/embedding_forward.svg)\n",
    "\n",
    "In the backward pass, the gradient will be used to adjust the embedding matrix, just like weights are updated in dense layers.  This means that tokens that have similar meanings will end up with vectors that are close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding layer\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the embedding weights\n",
    "        k = 1/math.sqrt(embed_dim)\n",
    "        self.weights =  torch.rand(vocab_size, embed_dim) * 2 * k - k\n",
    "        self.weights[0] = 0 # Zero out the padding embedding\n",
    "        # Using nn.Parameter tells torch to update this value in the backward pass\n",
    "        self.weights = nn.Parameter(self.weights)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # Return a matrix of embeddings, one row per token id\n",
    "        # The final shape will be (batch_size, token_count, embed_dim)\n",
    "        # We could convert token_ids to a one_hot vector and multiply by the weights, but it is the same as selecting a single row of the matrix\n",
    "        return self.weights[token_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the embedding vector for an individual token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0568,  0.0450,  0.0384,  0.0089,  0.0463,  0.0432,  0.0406, -0.0300,\n",
      "         0.0403, -0.0229], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "token_id = vocab[\"society\"]\n",
    "with torch.no_grad():\n",
    "    input_embed = Embedding(len(vocab), 256)\n",
    "    print(input_embed.weights[7][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at how embedding works for a batch in the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0145, -0.0502, -0.0270,  0.0477,  0.0186,  0.0531, -0.0056, -0.0390,\n",
      "         0.0407,  0.0004, -0.0343,  0.0387,  0.0329,  0.0418, -0.0130, -0.0592,\n",
      "        -0.0319, -0.0072, -0.0493, -0.0263])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(input_embed(batch[0])[0][0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the forward pass of the embedding layer, we end up with a 3-dimensional torch tensor with the shape `(B,T,E)`:\n",
    "\n",
    "- Dimension 0, `B`, is the batch dimension - one entry per element in the batch.  The length is the same as batch size.\n",
    "- Dimension 1, `T`, is the token dimension - one entry per input token.  The length is the number of input tokens (10).\n",
    "- Dimension 2, `E`, is the embedding dimension - one entry per element in the embedding vectors.  The length is the embedding dimension (256)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the next token\n",
    "\n",
    "We can now define a neural network that will predict the next token. It will be very similar to the networks we've built in past lessons.  The main difference will at the end of the network, when we make the final prediction.  We want the network to look at all tokens (the full sentence) when it predicts the next token.  To do this, we have to combine all embedding vectors into a single vector before the final layer.\n",
    "\n",
    "This network is doing classification, where the potential classes are the tokens in our vocabulary.  Our network will output the likelihood it assigns to the next token being each of the 11k items in our vocabulary.  We'll take the largest value as our prediction.\n",
    "\n",
    "The architecture will be:\n",
    "\n",
    "Start with a list of token ids.  The shape will be `(B, T)` where `B` is the batch size, and `T` is the number of tokens.\n",
    "\n",
    "- Embedding layer - from `(B,T)` to `(B,T,E)` where `E` is the embedding dimension\n",
    "- Dense layer - `(B,T,E)` to `(B,T,E)`\n",
    "- relu - nonlinear activation - `(B,T,E)` to `(B,T,E)`\n",
    "- Flatten - this will compress all token embeddings into one vector per batch element - `(B,T,E)` to `(B,T * E)`\n",
    "- Output layer - get the final token vector prediction -`(B,T * E)` to `(B,E)`\n",
    "- \"Unembed\" the vector - `(B,E)` to `(B,V)` where `V` is the vocabulary size\n",
    "\n",
    "![](images/text/network_embed.svg)\n",
    "\n",
    "We could apply softmax like we did in the [classification lesson](https://github.com/VikParuchuri/zero_to_gpt/blob/master/explanations/classification.ipynb) to get the probabilities that our next token is each token in the vocabulary.  But, the index of the largest element in the vector before and after the softmax will be the same (softmax preserves the relative order of the probabilities).  So we can just find the index of the largest element, and that will be our predicted token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, input_token_count, hidden_units):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        # Embed the token ids\n",
    "        self.embedding = Embedding(vocab_size, hidden_units)\n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Output layer looks at all embedding vectors and generates a prediction\n",
    "        self.output = nn.Linear(hidden_units * input_token_count, hidden_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed from (token_count, vocab_size) to (token_count, hidden_size)\n",
    "        embedded = self.embedding(x)\n",
    "        # Run the network\n",
    "        x = self.relu(self.dense1(embedded))\n",
    "        # Flatten the vectors into one large vector per sentence for the final layer\n",
    "        flat = torch.flatten(x, start_dim=1)\n",
    "        # Run the final layer to get an output\n",
    "        network_out = self.output(flat)\n",
    "        # Unembed, convert to (batch_size, vocab_size).  Argmax against last dim gives predicted token\n",
    "        out_vector = network_out @ self.embedding.weights.T\n",
    "        return out_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we define our network, we can write a training loop.\n",
    "\n",
    "We'll use [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) from PyTorch, since we're doing classification.  This works like the negative log likelihood that we covered in the [classification lesson](https://github.com/VikParuchuri/zero_to_gpt/blob/master/explanations/classification.ipynb).\n",
    "\n",
    "We'll make a prediction in the forward pass, measure loss, and then run the backward pass with the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# Initialize W&B\n",
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "def train_loop(net, optimizer, epochs):\n",
    "    # Initialize a new W&B run\n",
    "    wandb.init(project=\"text\",\n",
    "               name=\"dense\")\n",
    "\n",
    "    # We're doing classification, so we use crossentropy loss.\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch, (x, y) in enumerate(train):\n",
    "            # zero_grad will set all the gradients to zero\n",
    "            # We need this because gradients will accumulate in the backward pass\n",
    "            optimizer.zero_grad()\n",
    "            # Make a prediction using the network\n",
    "            pred = net(x)\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(pred, y)\n",
    "            # Call loss.backward to run backpropagation\n",
    "            loss.backward()\n",
    "            # Step the optimizer to update the parameters\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            if batch % 10 == 0:\n",
    "                # Log training metrics\n",
    "                wandb.log({\n",
    "                    \"train_loss\": mean(train_losses)\n",
    "                })\n",
    "\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our training loop, we can run our network.  We'll use regular SGD for our optimizer.  Adjust the number of epochs down if you want it to run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our hyperparameters\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize our network\n",
    "net = TokenPredictor(len(vocab), 10, 256)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "losses = train_loop(net, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the W&B dashboard to see the loss curve and other training information:\n",
    "\n",
    "![](images/text/loss_curve.png)\n",
    "\n",
    "\n",
    "The network isn't perfect, due to the architecture (more on that later).  You can try tweaking the parameters and layers to see if you can improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate predictions using our network, and compare to the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the society of his <ACTUAL>nephew<><PRED><UNK><>\n",
      "By a former marriage, Mr<ACTUAL>.<><PRED>,<>\n",
      "By his own marriage, likewise<ACTUAL>,<><PRED> <>\n",
      "But the fortune, which had<ACTUAL> <><PRED> <>\n",
      "But Mrs. John Dashwood was<ACTUAL> <><PRED> <>\n",
      "Marianne s abilities were, in<ACTUAL> <><PRED> <>\n",
      "She was sensible and clever  <ACTUAL>but<><PRED><UNK><>\n",
      "She was <UNK>, amiable, <ACTUAL>interesting<><PRED>that<>\n",
      "Elinor saw, with concern, <ACTUAL>the<><PRED>the<>\n",
      "They encouraged each other now <ACTUAL>in<><PRED><UNK><>\n",
      "The agony of grief which <ACTUAL>overpowered<><PRED><UNK><>\n",
      "Elinor, too, was deeply <ACTUAL>afflicted<><PRED><UNK><>\n",
      "A <UNK> in a place <ACTUAL>where<><PRED><UNK><>\n",
      "Mrs. John Dashwood did not<ACTUAL> <><PRED> <>\n",
      "To take three thousand pounds <ACTUAL>from<><PRED><UNK><>\n",
      "How could he answer it <ACTUAL>to<><PRED><UNK><>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch = next(iter(train))\n",
    "    pred = net(batch[0])\n",
    "    token_id = pred.argmax(-1)\n",
    "\n",
    "    for i in range(len(batch[0])):\n",
    "        text = decode(batch[0][i])\n",
    "        actual = decode(batch[1][i:(i+1)])\n",
    "        pred = decode(token_id[i:(i+1)])\n",
    "        print(f\"{text}<ACTUAL>{actual}<><PRED>{pred}<>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "In this lesson, we learned how to convert text into a representation that is appropriate for a neural network.  But the neural network we built isn't very accurate.  This is because it isn't using an optimal architecture.  Our dense network isn't able to look at relationships between tokens efficiently. We aren't able to scale the layers or data effectively as a result.\n",
    "\n",
    "The optimal architecture for predicting the next token is a transformer.  The good news is that we now have the building blocks we need to create a transformer model.  In the next lesson, we'll do exactly that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
